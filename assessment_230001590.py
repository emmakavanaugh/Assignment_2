# -*- coding: utf-8 -*-
"""Assessment_230001590.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14S6EjwyOPl0Tyn0dnyL-fty2uVlrzXqZ

# GG3209 Lab Assignment 2 - Python for Spatial Analysis

# Introduction
#### **Student ID:** 230001590
This notebook contains my solutions to the Python Basics, NumPy/Pandas, and Spatial Clustering exercises, as well as the final challenge.

# Part 1 - Python Basics

## 1.1. Testing Jupyter Notebooks
## Part 1 – Python Basics
In this section I complete the Python basics exercises (1.2–1.10).  
Each exercise includes a short explanation and the corresponding code/output.


* Create a Jupyter notebook and try the following operations based on the workbook:

* Create two or more code cells, and two or more markdown cells

* Type some code into the code cells, and execute it using **Ctrl+Enter**. You can try arithmetic expressions such as 10+5.

* For the Markdown cells you can try different types the Heading, lists, bullet point, bold text, and so on. What's important is you get familiar with the interface.  

* Delete one of the cells by pressing the D key twice.

In this section I practice creating and running code cells and Markdown cells in Jupyter Notebook, as well as basic operations like arithmetic and printing variables.

#### Example: Assigning and printing string variables

The code below shows how to assign string values to variables (including multiple assignment on one line) and print them using `print()`.
"""

x= "Hello"
print(x)
x,y,z = "How", "are", "you"
print(x, y, z)

"""#### Example: Basic arithmetic with variables

The code below assigns numbers to two variables and adds them together, demonstrating simple arithmetic operations in Python.

"""

a = 1
b = 2
a + b

"""## 1.2 Create a script that calculates the average of a list of numbers:

* In a new cell, create a list of numbers, for example: [80, 90, 95, 87, 82]
* Write a function that takes a list of numbers as input, calculates the average, and returns the result.
* Call the function with your list of numbers and print the result.


In this exercise I create a list of numeric values, calculate the average using `sum()` and `len()`, and print the result in a readable sentence.

"""

# Create a list of numbers
lst1 = [80, 90, 95, 87, 82]
# Calculate the average using sum / length of list
average = sum(lst1) / len(lst1)
# Print the result
print('The average is', average)

"""**Explanation:**  
The average is obtained by dividing the sum of all numbers in the list by the number of elements in the list.

## 1.3 Purpose of import statements

What is the purpose of the first two expressions. Explain what they do.

```
import pandas as pd
import geopandas as gpd
dat = pd.read_csv('data/world_cities.csv')  ## Import CSV file
geom = gpd.points_from_xy(dat['lon'], dat['lat'])
geom = gpd.GeoSeries(geom)
dat = gpd.GeoDataFrame(dat, geometry = gpd.GeoSeries(geom), crs = 4326)
dat.to_file('output/world_cities.shp')      ## Export Shapefile
```

**Explanation**: These lines import the pandas and geopandas libraries and give them the shorter aliases pd and gpd.
This does not install the libraries; it just makes their functions available in this script and lets us call them more easily (for example, pd.read_csv() instead of pandas.read_csv()).

## 1.4 Indentation

Python is sensitive when it comes to indentation.

Run the following code cell and see what happens and then try to address the issue:
"""

name = 'Dave'
dogs = 0
print('My name is', name, 'and I own', dogs, 'dogs.')

"""**How I addressed the issue:**  
Originally, the line `dogs = 0` was indented, which incorrectly made it look like part of a separate block.  
I removed the extra indentation so that `dogs = 0` is aligned with the other top-level statements. In Python, indentation should only be used to define blocks such as inside `if` statements, loops, or functions. Once the indentation was corrected, the code executed properly and printed the expected sentence.

## 1.5 Strings - Working with strings

In this exercise I combine string variables using the `+` operator and print them, and then include the combined string inside a longer sentence using `print()`.
"""

a = "Taco "
b = "time"
c = a + b
print(c)

"""In the following code cell,  I created a script where I printed a string and integer variables."""

a = "Taco "
b = "time"
c = a + b
print("Tuesday is", c)

"""**Explanation:**
In this exercise I combine string variables using the `+` operator and print them, and then include the combined string inside a longer sentence using `print()`.

## 1. 6 Nested *if* conditions


This example shows how to use nested `if` statements to print different messages depending on the season and the temperature.

The following example has some issues, address them and check the nested *if* conditional:
"""

season = "Winter"
temperature = 10

if season == "Winter":

     if temperature > 7:
         print("No need for winter jacket!")

     else:
         print("It might be cold! Wear a proper jacket!")

elif season == "Summer":

     if temperature > 20:
         print("It's warm! Time to wear shorts!")

     else:
         print("Well this is Finland, better wear long trousers!")
else:
     print("Check the weather forecast!")

"""#### My nested `if` example

In the following code I use nested `if` statements to check both the type of assessment and the grade, and print a different message depending on whether the student has passed.

"""

assessment = "Quiz"
grade = 75

if assessment == "Quiz":

     if grade > 60:
         print("You passed! Congratulations")

     else:
         print("Uh oh! You did not pass.")

elif assessment == "Project":

     if grade > 65:
         print("Your team did a great job, you passed!")

     else:
         print("Your team did not do well, you did not pass.")
else:
     print("Grades aren't out yet!")

"""## 1.7 Functions - Defining and Calling

Here we have the anatomy of any function in Python

![image.png](attachment:image.png)

source: pythongis.com

The example below defines a function that converts a temperature from Celsius to Fahrenheit and then calls the function with a sample value.



"""

def celsius_to_fahr(temp):
    return 9 / 5 * temp + 32
celsius_to_fahr(4)

"""#### My function: converting miles to kilometres

Here I define a function `miles_to_kms(dist)` that converts a distance in miles to kilometres, and then call it inside `print()` to display the result.

"""

def miles_to_kms(dist):
    return 1.61 * dist
print(miles_to_kms(4), 'miles')

"""**Explanation:**  
The function multiplies the input distance by 1.61 (approximate conversion factor) and returns the result. The `print()` statement shows the converted value together with the string `"miles"`.

## 1.8 Create a script that generates a multiplication table:

* Write a function that takes an integer as input and generates a multiplication table for that number, from 1 to 10.
* Call the function with a few different numbers and print the results.
* Optionally, you can use Markdown to format the output as a table.

In this exercise I define a function that takes an integer `n` as input and prints the multiplication table for `n` from 1 to 10 using a `for` loop.  
I then call the function with a couple of different values to demonstrate how it works.
"""

def multiplication_tbl(n):
    for i in range(1,11):
        print(n,'x',i,'=',n*i)

print('Multiplication table for 4:')
multiplication_tbl(4)

print('Multiplication table for 8:')
multiplication_tbl(8)

"""## 1.9 Counting items in a list with a loop and dictionary

* Define a list named *lunch*, as shown below.

* Use a **for loop** and **conditionals** to create a dictionary with the count of each item in lunch.

* **Hint**: Create an empty dictionary, then use a *for* loop to go over the items, and *a conditional* to add a new item with count 1 (if it is not yet in the dictionary), or to increment an existing item count.
* Write the code and print the result.

Here I create a list called `lunch` containing repeated food items.  
I then use a `for` loop and a dictionary to count how many times each unique item appears in the list.

"""

lunch = [
    'Salad',
    'Salad',
    'Egg',
    'Beef',
    'Potato',
    'Tea',
    'chicken',
    'Potato',
    'Potato',
    'Coffee'
]

counts = {}
for item in lunch:
    if item not in counts:
        counts[item] = 1
    else:
        counts[item] += 1
print(counts)

"""**Explanation:**  
If an item is not yet in the dictionary, I add it with count 1.  
If it already exists, I increase its count by 1. The final dictionary shows the frequency of each lunch item.

## 1.10 Reading and printing a CSV file

* Write in the following code cell, a script that reads and prints the **latest_earthquake_world.csv** file, included in the **data** folder.
* Use **pandas** module and print the first 30 rows.
* Hint: Use the second part notebook to get the required help.

In this exercise I use Python’s `csv` module and the `pandas` library to read the file `Latest_earthquake_world.csv` from the data folder and inspect its contents.  
Using `pandas.read_csv()`, I load the file into a DataFrame and display the first 30 rows with `.head(30)`.
"""

# This code reads data from external .zip files and extracts them into folders

import zipfile
import os

# Folder containing the zip files
folder_with_zips = '/content/drive/MyDrive/GG3209/'

# Ensure the output directory exists
output_dir = '/content/drive/MyDrive/GG3209'
os.makedirs(output_dir, exist_ok=True)  # Create folder if it does not exist

# Iterate through all files in the folder
for filename in os.listdir(folder_with_zips):
    file_path = os.path.join(folder_with_zips, filename)

    # Only process .zip files
    if filename.endswith('.zip'):
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            # Create a folder with the same name as the zip file (without the .zip extension)
            folder_name = os.path.splitext(filename)[0]
            folder_path = os.path.join(output_dir, folder_name)
            os.makedirs(folder_path, exist_ok=True)

            # Extract all contents of the zip file into the folder
            zip_ref.extractall(folder_path)

            print(f'Oh look what I got, Unzipped {filename} to {folder_path}')
    else:
        # Skip non-zip files
        print(f'Skipping {filename} (Sorry, not a zip file)')

print("Unzipping completed. Wahoo!")

txt1 = "D:\\data\\text_files\\t1.txt"
dataset = '/content/drive/MyDrive/GG3209/data/data/Latest_earthquake_world.csv'

import csv
with open(dataset, "r") as csvfile:
    reader = csv.reader(csvfile)


import numpy as np
import pandas as pd
Latest_earthquake_world_df = pd.read_csv(dataset, sep=",", header=0, encoding="ISO-8859-1")
Latest_earthquake_world_df.head(30)

"""# Python for Spatial Analysis
## Part 2 - Notebook to practice NumPy and Pandas - Exercises

# Practicing NumPy

### 2.0 Importing NumPy

Here I import the NumPy library with the alias `np`, which is the standard convention in Python.
"""

import numpy as np

"""#### 2.1 Creating basic NumPy arrays

In this exercise I:

- Create a 1D array of 10 ones.  
- Create an array of integers from 1 to 20.  
- Create a 5 × 5 matrix of ones with integer (`int`) data type.

These examples show how to use `np.ones()` and `np.arange()` and how to specify shape and `dtype`.

"""

arr_ones = np.ones(10)
arr_ints = np.arange(1, 21)
matrix_ones = np.ones((5, 5), dtype=int)

arr_ones, arr_ints, matrix_ones

"""### 2.2 Creating and reshaping a 3D array

Here I:

- Create a 3 × 3 × 3 array of random numbers drawn from a standard normal distribution using `np.random.randn()`.  
- Reshape this 3D array into a 1D array of length 27 using `.reshape(27,)`.

This demonstrates how to generate random data and change the shape of a NumPy array.

"""

arr_3d = np.random.randn(3, 3, 3)
arr_flat = arr_3d.reshape(27,)

arr_3d, arr_flat

"""### 2.3 Linearly spaced values

In this cell I use `np.linspace(1, 10, 20)` to create an array of 20 evenly spaced numbers between 1 and 10 (inclusive).

"""

np.linspace(1, 10, 20)

"""### 2.4 Indexing and slicing a 2D array

I first create a 5 × 5 array `a` with values from 1 to 25 using `np.arange()` and `.reshape()`.  
Then I use indexing and slicing to extract specific elements and sub-arrays.

"""

import numpy as np
a = np.arange(1, 26).reshape(5, -1)

20

"""#### Extracting a single element

Here I access the element in row 3, column 4 (0-based indexing), which returns the value 20.

"""

a = np.arange(1, 26).reshape(5, -1)
a[3, 4]

"""```python
array([[ 9, 10],
       [14, 15],
       [19, 20],
       [24, 25]])
```

#### Extracting a sub-array

In this cell I slice the array from row 1 to the end and from column 3 to the end, producing a 4 × 2 sub-array.
"""

a = np.arange(1, 26).reshape(5, -1)
output = a[1:, 3:]
output

"""```python
array([ 6,  7,  8,  9, 10])
```

#### Selecting a full row

Here I select the second row of the array `a` (row index 1), which returns a 1D array containing five elements.
"""

a = np.arange(1, 26).reshape(5, -1)
a[1]

"""### 2.5 Sum of all elements in an array

In this cell I calculate the sum of all values in `a` using `a.sum()`.  
This returns the total of all numbers from 1 to 25.

"""

a = np.arange(1, 26).reshape(5, -1)
a.sum()

"""### 2.6 Row-wise sums

Here I compute the sum of each row in the array `a` using `a.sum(axis=1)`.  
The result is a 1D array where each value is the total of one row.

"""

a = np.arange(1, 26).reshape(5, -1)
a.sum(axis=1)

"""### 2.7 Boolean masking with the mean

In this exercise I:

- Calculate the mean of all values in `a` using `a.mean()`.  
- Create a boolean mask `a > mean_val`.  
- Use this mask to extract all values in `a` that are greater than the mean.

This demonstrates how to filter arrays using boolean indexing.

"""

a = np.arange(1, 26).reshape(5, -1)
mean_val = a.mean()
a[a > mean_val]

"""# Practicing Pandas

### 2.8 Importing pandas

In this we'll be investigating the carbon footprint of different foods.

We'll be leveraging a dataset compiled by [Kasia Kulma](https://r-tastic.co.uk/post/from-messy-to-tidy/) and contributed to [R's Tidy Tuesday project](https://github.com/rfordatascience/tidytuesday).

I started by importing pandas with the alias `pd`, which is the convention.
"""

import pandas as pd

"""### 2.9 Loading the food consumption dataset

The dataset we'll be working with has the following columns:

|column      |description |
|:-------------|:-----------|
|country       | Country Name |
|food_category | Food Category |
|consumption   | Consumption (kg/person/year) |
|co2_emmission | Co2 Emission (Kg CO2/person/year) |


Import the dataset as a dataframe named `df` from this url: <https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv>

In this cell I read the `food_consumption.csv` file into a Pandas DataFrame named `df` using `pd.read_csv()`.  
The dataset contains information on food consumption and CO₂ emissions by country and food category.
"""

df = pd.read_csv('/content/drive/MyDrive/GG3209/food_consumption[1].csv')

"""### 2.10 Number of rows and columns

Here I use `df.shape` to find out how many rows and columns are in the dataset and print these values.

"""

rows, cols = df.shape
print('Rows:', rows)
print('Columns:', cols)

"""### 2.11 Mean CO₂ emission of the dataset

In this cell I compute the overall mean of the `co2_emmission` column using `.mean()` and print the result.

"""

df = pd.read_csv('/content/drive/MyDrive/GG3209/food_consumption[1].csv')
mean_co2 = df['co2_emmission'].mean()
print('Mean CO2 emissions', mean_co2)

"""### 2.12 Maximum CO₂ emission – food type and country

Here I:

- Find the row with the maximum value in `co2_emmission` using `idxmax()` and `.loc`.  
- Extract the maximum emission value, the corresponding `food_category`, and the `country`.  
- Print these values in a readable format.

"""

# Find the row with the maximum CO2 emission
max_row = df.loc[df['co2_emmission'].idxmax()]

# Extract the values we care about
max_co2 = max_row['co2_emmission']
food_type = max_row['food_category']
country = max_row['country']

# Print a clear summary
print("Maximum CO2 emission:", max_co2)
print("Food type:", food_type)
print("Country:", country)

"""### 2.13 Countries producing more than 1000 kg CO₂/year for at least one food type

In this exercise I:

- Filter the DataFrame to rows where `co2_emmission` is greater than 1000.  
- Use `.nunique()` on the `country` column to count how many different countries appear in this filtered subset.  
- Print the number of countries.

"""

highest_emissions = df[df['co2_emmission']> 1000]
num_countries = highest_emissions['country'].nunique()
print('Number of countries:', num_countries)

"""### 2.14 Country with the lowest beef consumption

Here I:

- Filter the DataFrame to rows where `food_category` is Beef.  
- Use `idxmin()` on the `consumption` column to find the row with the smallest beef consumption.  
- Print the country name and the corresponding consumption value (kg/person/year).

"""

df['food_category'].unique()
Beef = df[df['food_category'] == 'Beef']

min_row = Beef.loc[Beef['consumption'].idxmin()]

Beef = df[df['food_category'].str.lower() == 'Beef']

print("Country with the lowest beef consumption:", min_row['country'])
print("Beef consumption (kg/person/year):", min_row['consumption'])

"""### 2.15 Total emissions from meat products

In this exercise I:

- Define a list of meat categories: Pork, Poultry, Lamb & Goat, Fish, and Beef.  
- Filter the DataFrame to include only these categories.  
- Sum the `co2_emmission` values for these rows to obtain the total CO₂ emissions from all meat products in the dataset.

"""

meat_products = ['Pork', 'Poultry', 'Lamb & Goat', 'Fish', 'Beef']

meat_df = df[df['food_category'].isin(meat_products)]

total_meat_emissions = meat_df['co2_emmission'].sum()

print('Total CO2 emissions from all meat products in the dataset:', total_meat_emissions)

"""### 2.16 Total emissions from non-meat products

Here I compute:

- `total_emissions`: the sum of `co2_emmission` for all rows in the dataset.  
- `total_meat_emissions`: the value calculated in the previous exercise.  
- `total_non_meat`: the remaining emissions by subtracting meat emissions from the total.

This gives the combined CO₂ emissions of all non-meat food categories in the dataset.

"""

total_emissions = df['co2_emmission'].sum()

total_non_meat = total_emissions - total_meat_emissions

print('Total emissions of all other (non-meat) products in the dataset combined:', total_non_meat)

"""## Part 3 – Spatial Clustering with K-Means and DBSCAN

In this section I work with the UK road accident dataset to:
- Perform exploratory data analysis (EDA) on accidents from 2010 onward.
- Visualise temporal and environmental patterns (e.g. day of week, road surface conditions).
- Build GeoDataFrames for spatial analysis.
- Apply K-Means and DBSCAN clustering to identify accident hotspots in:
  - The Glasgow–Edinburgh corridor.
  - Birmingham.
- Explore spatial autocorrelation using Moran’s I.

#### Getting the required data in your Drive

I went to [Kaggle - Car Accidents in the UK](https://www.kaggle.com/datasets/devansodariya/road-accident-united-kingdom-uk-dataset/) and downloaded the **Road Accident (United Kingdom (UK)) dataset**.

I uploaded the dataset to my Google Drive and mounted my drive in this Notebook so I could access the data in the following tasks.


---
"""

from google.colab import drive  # Import function to mount Google Drive
drive.mount('/content/drive', force_remount=True)  # Mount Drive at /content/drive

# Install lonboard, a library for visualising large geospatial datasets in notebooks
!pip install lonboard

"""### 3.2 Importing core libraries

Here I import:
- Numerical and tabular libraries (`numpy`, `pandas`)
- Spatial libraries (`geopandas`, `shapely`, `folium`)
- Clustering tools from `scikit-learn`
- Plotting libraries (`matplotlib`, `seaborn`, `plotly`)
- Mapping tools from `lonboard`

"""

# Core numerical and tabular libraries
import numpy as np
import pandas as pd

# Spatial libraries
import geopandas as gpd
from shapely.geometry import Point

# Clustering algorithms
from sklearn.cluster import KMeans, DBSCAN

# Plotting / visualisation libraries
import matplotlib.pyplot as plt
import seaborn as sns
import folium

# Lonboard for fast, interactive web maps
from lonboard import Map, ScatterplotLayer, viz

"""### 3.3 Unzipping the UK accident dataset

The raw data is stored inside a `.zip` archive in my Google Drive.  
In this step I:
- Loop over all zip files in the GG3209 folder.
- Extract each archive to a new folder with the same base name.

"""

# This code reads data from external .zip files and extracts them into folders

import zipfile
import os

# Folder containing the zip files
folder_with_zips = '/content/drive/MyDrive/GG3209/'

# Ensure the output directory exists
output_dir = '/content/drive/MyDrive/GG3209'
os.makedirs(output_dir, exist_ok=True)  # Create folder if it does not exist

# Iterate through all files in the folder
for filename in os.listdir(folder_with_zips):
    file_path = os.path.join(folder_with_zips, filename)

    # Only process .zip files
    if filename.endswith('.zip'):
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            # Create a folder with the same name as the zip file (without the .zip extension)
            folder_name = os.path.splitext(filename)[0]
            folder_path = os.path.join(output_dir, folder_name)
            os.makedirs(folder_path, exist_ok=True)

            # Extract all contents of the zip file into the folder
            zip_ref.extractall(folder_path)

            print(f'Oh look what I got, Unzipped {filename} to {folder_path}')
    else:
        # Skip non-zip files
        print(f'Skipping {filename} (Sorry, not a zip file)')

print("Unzipping completed. Wahoo!")

"""### 3.4 Loading the accident dataset and selecting relevant attributes

Here I:
- Load the full UK accident dataset into a pandas DataFrame.
- Inspect the columns.
- Keep only a subset of relevant variables for analysis (location, severity, conditions, temporal attributes).

"""

# Load the UK accident dataset from the unzipped archive
accident_data = pd.read_csv('/content/drive/MyDrive/GG3209/archive/UK_Accident.csv')
# Using pandas to read the CSV file from Google Drive

#This code visualizes the first few rows to understand the available attributes and ensure the .csv uploaded properly
accident_data.head()

# List all available columns in the dataset
accident_data.columns.values.tolist()

# The list of selected columns to keep from the larger datase
keep_cols = [
    'Longitude',
    'Latitude',
    'Accident_Severity',
    'Number_of_Vehicles',
    'Number_of_Casualties',
    'Light_Conditions',
    'Weather_Conditions',
    'Road_Surface_Conditions',
    'Urban_or_Rural_Area',
    'Date',
    'Day_of_Week',
    'Year']
# Subset the DataFrame to only the selected columns
accident_data = accident_data [keep_cols]
# Check the resulting trimmed DataFrame
accident_data. head ()

"""### 3.5 Filtering to accidents from 2010 onwards and parsing dates

To reduce the dataset size and focus on a recent period, I:
- Keep only accidents from 2010 onwards.
- Convert the `Date` column to a proper datetime type.
- Derive the name of the day of the week for plotting.

"""

# Filter accidents to records from 2010 onwards
df_2010 = accident_data[accident_data['Year'] >= 2010].copy()

# Check number of rows and columns in the filtered dataset
df_2010.shape

# Convert 'Date' to datetime format (dayfirst=True for UK-style dates)
df_2010['Date'] = pd.to_datetime(df_2010['Date'], dayfirst=True)

# Extract day names (Monday, Tuesday, etc.) from the datetime column
df_2010['Day_of_Week'] = df_2010['Date'].dt.day_name()

# Count accidents per day in a fixed order
counts = df_2010['Day_of_Week'].value_counts().reindex([
    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'
])

# Simple bar plot of accidents by day of the week
plt.figure(figsize=(8, 4))
counts.plot(kind='bar')
plt.title("Number of Car Accidents by Day of the Week")
plt.xlabel("Day of Week")
plt.ylabel("Number of Accidents")
plt.tight_layout()
plt.show()

"""###Based on this graph what insights can you gain about the relationship between Accident Severity and Road Conditions?
This chart shows clear weekly patterns in accident frequency. Typically, the highest number of accidents occurs on Fridays, likely due to increased traffic volumes, end-of-week commuting, and more evening social travel. Weekends usually show a slight drop or shift in accident distribution depending on the dataset. This pattern suggests that temporal factors, especially weekday travel routines, play a significant role in accident risk.

### 3.6 Accident severity by road surface condition

Next, I explore how accident severity varies with road surface conditions using a stacked bar chart.
"""

# Count the number of accidents by combination of road surface and accident severity
severity_road_counts = df_2010.groupby(
    ['Road_Surface_Conditions', 'Accident_Severity']
).size().unstack(fill_value=0)

# Plot a stacked bar chart of severity by road surface condition
plt.figure(figsize=(10, 6))
severity_road_counts.plot(kind='bar', stacked=True, figsize=(10, 6))

plt.title("Accident Severity by Road Surface Conditions")
plt.xlabel("Road Surface Condition")
plt.ylabel("Number of Accidents")
plt.legend(title='Accident Severity')
plt.tight_layout()
plt.show()

"""###Based on this graph what insights can you gain about the relationship between Accident Severity and Road Conditions?
This second chart highlights how environmental factors relate to severity. Most accidents occur on dry roads, which is expected because they represent the majority of driving conditions. However, wet, icy, or snowy road conditions often show a higher proportion of serious or fatal accidents, even if the total number is smaller. This suggests that adverse surface conditions may increase the likelihood that an accident becomes more severe. Drivers may be less prepared for sudden loss of traction, braking challenges, or reduced visibility, amplifying the consequences of a crash.

###Based on both graphs what insights can you gain about the relationship between Accident Severity and Road Conditions?
Overall the two charts indicate that accident frequency is shaped heavily by human mobility patterns (such as weekday travel peaks), whereas accident severity is influenced more by environmental and road condition factors. This combination shows the importance of considering both temporal and environmental variables when analysing spatial accident clusters or designing safety interventions.

### 3.7 Creating a GeoDataFrame and mapping all accidents (2010+)

Here I:
- Convert the longitude/latitude columns to a geometry column of `Point` objects.
- Build a GeoDataFrame with CRS WGS84 (`EPSG:4326`).
- Visualise all accidents using `lonboard`.
"""

# This creates a GeoSeries of Shapely Point objects.
# Each Point is built from a pair of coordinates (Longitude = x, Latitude = y),
# converting the numeric lon/lat columns into actual spatial geometry.
geometry = gpd.points_from_xy(df_2010['Longitude'], df_2010['Latitude'])

# This constructs a GeoDataFrame by attaching the geometry column to the filtered dataset.
# The CRS defines how the coordinates should be interpreted and ensures correct mapping.
gdf_2010 = gpd.GeoDataFrame(
    df_2010,
    geometry=geometry,
    crs="EPSG:4326"    # Assign the coordinate reference system (WGS84 latitude/longitude)
)

from lonboard import viz

viz(gdf_2010) # Display interactive map of all accidents (2010+)

"""### 3.8 Filtering accidents to the Glasgow–Edinburgh region

To focus on a specific corridor, I:
- Define a bounding box around the Glasgow–Edinburgh area.
- Subset the GeoDataFrame to accidents within this box.
- Display the filtered accidents using `lonboard`.

"""

# Define a bounding box around the Glasgow–Edinburgh region
min_lon, max_lon = -4.5, -3.0
min_lat, max_lat = 55.7, 56.1
# Approx:
#   Glasgow  ≈ (-4.25, 55.86)
#   Edinburgh ≈ (-3.19, 55.95)

# Filter accidents that fall within the bounding box
gdf_ge = gdf_2010[
    (gdf_2010['Longitude'].between(min_lon, max_lon)) &
    (gdf_2010['Latitude'].between(min_lat, max_lat))
].copy()

viz(gdf_ge) # Display interactive map of accidents only in the Glasgow–Edinburgh region

"""### 3.9 K-Means clustering on accident coordinates (Glasgow–Edinburgh)

In this step I:
- Apply K-Means clustering to the Glasgow–Edinburgh subset using only longitude and latitude.
- Start with `k = 5` clusters.

"""

# Apply K-Means using only spatial coordinates (longitude, latitude)
kmeans = KMeans(n_clusters=5, random_state=42)

# Fit the model and assign cluster labels (0–4)
gdf_ge['kmeans_cluster'] = kmeans.fit_predict(gdf_ge[['Longitude', 'Latitude']])

"""### 3.10 Projecting to a metric CRS and applying DBSCAN

DBSCAN requires a distance-based notion of neighbourhood (in metres).  
Here I:
- Reproject the Glasgow–Edinburgh GeoDataFrame to British National Grid (`EPSG:27700`).
- Extract projected x/y coordinates.
- Run DBSCAN with a chosen `eps` (radius in metres) and `min_samples` (minimum points per cluster).

"""

# Check the current CRS
print(gdf_ge.crs)

# Reproject to a metric CRS (British National Grid, EPSG:27700)
gdf_ge_projected = gdf_ge.to_crs(epsg=27700)

# Extract x and y coordinates from the projected geometry
gdf_ge_projected['geometry_x'] = gdf_ge_projected.geometry.x
gdf_ge_projected['geometry_y'] = gdf_ge_projected.geometry.y

# Set DBSCAN parameters
meters_eps = 200      # Neighbourhood radius (in metres)
min_samples_val = 40  # Minimum number of accidents required to form a cluster

# Initialise DBSCAN with the chosen parameters
dbscan = DBSCAN(eps=meters_eps, min_samples=min_samples_val)

# Run DBSCAN on the projected coordinates
gdf_ge_projected['dbscan_cluster'] = dbscan.fit_predict(
    gdf_ge_projected[['geometry_x', 'geometry_y']]
)

# Preview the cluster labels
gdf_ge_projected['dbscan_cluster'].value_counts()

import plotly.express as px

# Plot DBSCAN clusters for Glasgow–Edinburgh using Plotly
fig_dbscan = px.scatter_mapbox(
    gdf_ge_projected,
    lat="Latitude",
    lon="Longitude",
    color="dbscan_cluster",
    mapbox_style="carto-positron",
    zoom=9,
    title="DBSCAN Clustering when KMean=5",
    height=700,
    opacity=0.5,

)
fig_dbscan.show()

"""### 3.11 K-Means clustering using spatial and accident attributes

Here I extend K-Means to use:
- Spatial coordinates (Longitude, Latitude)
- Accident attributes (Accident_Severity, Number_of_Vehicles)

This allows the clusters to reflect both location and accident characteristics.

"""

# Selected a different numeric value to use for K-means to demonstrate how changing K-Means,
# changes the spatial clustering and the map
kmeans_varient = KMeans(n_clusters=3, random_state=42)
gdf_ge['kmeans_cluster'] = kmeans_varient.fit_predict(gdf_ge[['Longitude', 'Latitude']])

# Projected to a metric system (meters) for DBSCAN
gdf_ge_projected = gdf_ge.to_crs(epsg=27700)

# Extracted X and Y coordinates (in meters) to use with DBSCAN
gdf_ge_projected['geometry_x'] = gdf_ge_projected.geometry.x
gdf_ge_projected['geometry_y'] = gdf_ge_projected.geometry.y

meters_eps = 200 # Radius of 200 meters
min_samples_val = 40  # Minimum number of accidents to form dense cluster

# Initialized the DBSCAN clustering algorithm using two parameters, EPS & min_samples
dbscan = DBSCAN(eps=meters_eps, min_samples=min_samples_val)

# Stored the resulting cluster labels (or noise points) in a new column called dbscan_cluster
gdf_ge_projected['dbscan_cluster'] = dbscan.fit_predict(
    gdf_ge_projected[['geometry_x', 'geometry_y']])
fig_dbscan = px.scatter_mapbox(
    gdf_ge_projected,
    lat="Latitude",
    lon="Longitude",
    color="dbscan_cluster",
    mapbox_style="carto-positron",
    zoom=9,
    title="DBSCAN Clustering when KMean = 3",
    height=700,
    opacity=0.5,

)
# Plotted data using function px.scatter_mapbox from the Plotly library
fig_dbscan.show()

"""###How does the choice of k impact the clusters? Describe how the clusters change once you adjust multiple versions of that required parameter.
The K-means clustering results show that the choice of k has a major impact on how accidents are grouped spatially. With k = 3, the clusters form broad geographic zones that typically separate Glasgow, Edinburgh, and the corridor between them. These clusters capture high-level regional structure but may overlook smaller hotspots. When k = 5, the clusters become more granular, revealing localised concentrations of accidents around motorway interchanges, dense city centres, and key A-road networks. Increasing k therefore provides more detail but can also introduce noise, as K-means will always divide the data into exactly k groups—even when the spatial pattern does not naturally support that number of clusters.
In contrast, DBSCAN does not require specifying the number of clusters in advance. Instead, it identifies clusters based on density, using eps (distance radius) and min_samples (minimum neighbouring points). When eps is small, only very tight and dense hotspots appear, and many points are labelled as noise. A moderate eps value (e.g., 200 m) reveals meaningful spatial clusters in urban areas and along major road corridors. Larger eps values cause distinct hotspots to merge together, reducing interpretability. Similarly, lowering min_samples produces many small clusters, while higher values highlight only major, persistent accident hotspots. Very high thresholds may classify nearly all points as noise.
Overall, K-means is useful for understanding broad spatial patterns, but its results depend entirely on the chosen number of clusters. DBSCAN, on the other hand, finds natural accident hotspots driven by spatial density and is particularly effective for identifying areas with consistently high accident concentrations. Together, the two methods provide complementary insights into accident distribution across the Glasgow–Edinburgh region.
"""

X_attr = gdf_ge[['Longitude', 'Latitude', 'Accident_Severity', 'Number_of_Vehicles']]
# Selected features for K-means (space + accident attributes)
# Using Longitude, Latitude, Accident_Severity, Number_of_Vehicles

kmeans_attr = KMeans(n_clusters=5, random_state=42)
# Fitted K-means with k = 5 on the selected attributes

gdf_ge['kmeans_attr_cluster'] = kmeans_attr.fit_predict(X_attr)
# This created a new column 'kmeans_attr_cluster' with cluster labels (0–4)

from lonboard import Map, ScatterplotLayer
from lonboard.colormap import apply_categorical_cmap


cluster_cmap = {
    0: [230, 25, 75, 220],   # red
    1: [60, 180, 75, 220],   # green
    2: [0, 130, 200, 220],   # blue
    3: [255, 225, 25, 220],  # yellow
    4: [128, 0, 128, 220],   # purple
} # Built a categorical colormap for the 5 clusters


cluster_colors = apply_categorical_cmap(
    gdf_ge['kmeans_attr_cluster'].to_numpy(),
    cmap=cluster_cmap
) # Converted cluster labels to RGBA colors using Lonboard's colormap helper

layer_kmeans_attr = ScatterplotLayer.from_geopandas(
    gdf_ge,
    get_fill_color=cluster_colors,
    radius_min_pixels=3,
) # Created a Lonboard ScatterplotLayer colored by the K-means attribute clusters

Map(layer_kmeans_attr) # Displayed the interactive map with accidents coloured by their K-means (attribute-based) cluster

"""###Reflect on the clusters that include only the coordinates and the ones that also include other attributes. What insights can you gain about that?
Clustering based only on longitude and latitude highlights the purely spatial structure of the accident data. These clusters tend to group accidents into geographic regions such as the Glasgow area, the Edinburgh area, and the road networks between them. The results mainly reflect physical proximity and the distribution of traffic across the road network. This approach is useful for identifying broad spatial patterns and regions where accidents naturally cluster together, but it cannot distinguish between areas that differ in accident severity or the number of vehicles involved.
When additional attributes such as Accident Severity and Number of Vehicles are included, the clusters begin to represent not only where accidents occur, but also what kind of accidents occur in each area. As a result, accidents that may be geographically close can be separated into different clusters if they differ significantly in severity or complexity. For example, a major motorway corridor may contain both high-severity multi-vehicle crashes and more frequent low-severity incidents; in the attribute-based clustering, these may fall into different groups even though they are spatially close. Conversely, accidents in different parts of the region may be grouped together if they share similar characteristics, revealing patterns that the coordinate-only approach cannot capture.
Overall, incorporating additional attributes provides a richer and more nuanced understanding of accident patterns. Coordinate-only clustering is effective for identifying spatial hotspots, while attribute-enhanced clustering highlights differences in accident types, severity levels, and traffic conditions. The combination of both approaches offers a more complete picture of how accidents are distributed and what factors may contribute to different types of incidents across the Glasgow–Edinburgh region.

# 3. Spatial Analysis and DBSCAN Clustering

## Part A: Spatial Correlation

### 3.12 Create another GeoPandas Dataframe by rereading the data to avoid any confusion with the previous geodataframe. This new one is about DBSCAN name it accordingly.
"""

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point

"""### 3.13 Using the BBox website, filter the new geodataframe to contain only the accidents around Birmingham."""

# Re-read the original UK accidents dataset
df_dbscan = pd.read_csv('/content/drive/MyDrive/GG3209/archive/UK_Accident.csv')

# Bounding box around Birmingham (from BBox website)
min_lon, max_lon = -2.05, -1.75
min_lat, max_lat = 52.35, 52.55

# Filter accidents to the Birmingham bounding box
df_birmingham_filtered = df_dbscan[
    (df_dbscan["Longitude"].between(min_lon, max_lon)) &
    (df_dbscan["Latitude"].between(min_lat, max_lat))
].copy()

# Create geometry column and build GeoDataFrame
geometry = gpd.points_from_xy(df_birmingham_filtered['Longitude'], df_birmingham_filtered['Latitude'])
gdf_birmingham = gpd.GeoDataFrame(
    df_birmingham_filtered,
    geometry=geometry,
    crs="EPSG:4326"
)

# Preview the filtered dataset
gdf_birmingham.head()

"""### 3.13 Using the Lonboard library, map the filtered dataset in **Birmingham**."""

from lonboard import Map, ScatterplotLayer

# Created a ScatterplotLayer from the gdf_birmingham GeoDataFrame
layer_birmingham = ScatterplotLayer.from_geopandas(gdf_birmingham)

# Display the interactive map with accidents in Birmingham
Map(layer_birmingham)

"""### 3.14 In a code cell, investigate the data type of the attribute list, so you can identify which attributes are numerical and which are categorical."""

# Inspect attribute types to distinguish numerical vs categorical variables
print(gdf_birmingham.dtypes)

"""###3.15 Run the correlation between the numerical attributes by including in your code corr= your_dataframe.corr()"""

import libpysal.weights as weights
from esda.moran import Moran

# Reproject the dataset to a metric system suitable for the UK (EPSG:27700)
gdf_birmingham_projected = gdf_birmingham.to_crs(epsg=27700)
print(gdf_birmingham_projected.crs)
gdf_birmingham_projected.head()

birmingham_corr = gdf_birmingham.corr(numeric_only=True)
# This code computes the correlation matrix for all numerical attributes in the DataFrame.
print(birmingham_corr.head())

"""### 3.16 Adjust the following code to create a heatmap plot of your correlation values."""

plt.figure(figsize=(10, 8))
sns.heatmap(
    birmingham_corr,
    annot=True,
    cmap='coolwarm',
    fmt='.2f',           # Format annotations to 2 decimal places
    linewidths=0.5,
    cbar_kws={'label': 'Correlation Coefficient'}
)

plt.title('Pearson -Correlation Matrix')
plt.show()

"""### 3.17 Install the library **pysal** by running in a code cell 'pip install pysal'"""

pip install pysal

"""### 3.18 Import the new and requiered libraries."""

w = weights.DistanceBand.from_dataframe(gdf_birmingham_projected, threshold=500, ids=gdf_birmingham.index)
w.transform = 'R'
moran = Moran(gdf_birmingham['Accident_Severity'], w)

print(f"\n--- Moran's I Spatial Autocorrelation Analysis ---")
print(f"Defined {w.n} observations and {w.mean_neighbors:.2f} average neighbors per point.")
print(f"\nMoran's I Statistic (Observed I): {moran.I:.4f}")
print(f"P-value (significance): {moran.p_sim:.4f}")

"""### 3.19 Describe with your own words the results. What insights can you gain from the correlation analysis?
A Moran's I value close to 0 (like 0.0048) suggests a random spatial pattern. This means there is very little to no evidence of clustering or dispersion of accident severity based on the defined spatial neighborhood (within 500 meters).
The p-value of 0.0010 is statistically significant (typically, a p-value < 0.05 is considered significant). This means that even though the Moran's I value is close to zero, there is a statistically significant difference from a perfectly random distribution. However, given the very small I value, this significance doesn't point to strong clustering, but rather a very weak, yet non-random, pattern.
The warnings about 'islands' (18 observations with no neighbors) indicate that some accident locations were too isolated to have any other accidents within the 500-meter threshold. This can happen in sparser areas of the dataset and suggests that the chosen threshold might be too small for some points, making it hard to form a complete network of spatial relationships.
In essence, while the pattern isn't perfectly random, the extremely low Moran's I value suggests that accident severity in Birmingham, at this 500m scale, does not show strong spatial clustering or dispersion.

## Part B: DBSCAN Clustering Implementation:

###3.21 Implement DBSCAN clustering with different eps and min_samples to the projected dataset.
"""

from sklearn.cluster import DBSCAN

gdf_birmingham_projected['geometry_x'] = gdf_birmingham_projected.geometry.x
gdf_birmingham_projected['geometry_y'] = gdf_birmingham_projected.geometry.y
# Extract projected x/y coordinates into new columns

X = gdf_birmingham_projected[['geometry_x', 'geometry_y']]
# Select projected x/y coordinates in meters

eps_val = 150          # NEW: neighborhood radius in meters (reduced from 200)
min_samples_val = 60   # NEW: minimum number of points for a cluster (increased from 40)
# Define DBSCAN parameters (edit these as needed)

dbscan = DBSCAN(eps=eps_val, min_samples=min_samples_val)
# Run DBSCAN on the projected data and store results in gdf_birmingham_projected
gdf_birmingham_projected['dbscan_cluster'] = dbscan.fit_predict(X)

gdf_birmingham_projected['dbscan_cluster'].value_counts()
# Preview results

"""### 3.22 Map the clusters using the Plotly Library."""

import plotly.express as px

fig_dbscan = px.scatter_mapbox(
    gdf_birmingham_projected,
    lat="Latitude",
    lon="Longitude",
    color="dbscan_cluster", # Use the newly created dbscan_cluster for coloring
    mapbox_style="carto-positron",
    zoom=10, # Adjusted zoom level for Birmingham
    title="DBSCAN Clustering of Accidents in Birmingham (eps=150, min_samples=60)", # Updated title
    height=700,
    opacity=0.5,
    hover_data=['Accident_Severity', 'Number_of_Vehicles', 'Road_Surface_Conditions']
)
fig_dbscan.show()

"""### Repeat step 3.21 with different eps and min_samples to the projected dataset."""

from sklearn.cluster import DBSCAN

eps_small = 150
dbscan_small = DBSCAN(eps=eps_small, min_samples=30)
gdf_birmingham_projected['dbscan_small'] = dbscan_small.fit_predict(X)
# Example: Smaller eps (tighter clusters)

eps_large = 300
dbscan_large = DBSCAN(eps=eps_large, min_samples=60)
gdf_birmingham_projected['dbscan_large'] = dbscan_large.fit_predict(X)
# Example: Larger eps (broader clusters)

"""### Repeat step 3.22, mapping new variations of the clusters using the Plotly Library."""

import plotly.express as px

fig_dbscan_small = px.scatter_mapbox(
    gdf_birmingham_projected,
    lat="Latitude",
    lon="Longitude",
    color="dbscan_small", # Use the dbscan_small clusters for coloring
    mapbox_style="carto-positron",
    zoom=10,
    title="DBSCAN Clustering of Accidents in Birmingham (eps=150, min_samples=30)",
    height=700,
    opacity=0.5,
    hover_data=['Accident_Severity', 'Number_of_Vehicles', 'Road_Surface_Conditions']
)
fig_dbscan_small.show()

"""### Repeat step 3.22, mapping new variations of the clusters using the Plotly Library."""

import plotly.express as px

fig_dbscan_large = px.scatter_mapbox(
    gdf_birmingham_projected,
    lat="Latitude",
    lon="Longitude",
    color="dbscan_large", # Use the dbscan_large clusters for coloring
    mapbox_style="carto-positron",
    zoom=10,
    title="DBSCAN Clustering of Accidents in Birmingham (eps=300, min_samples=60)",
    height=700,
    opacity=0.5,
    hover_data=['Accident_Severity', 'Number_of_Vehicles', 'Road_Surface_Conditions']
)
fig_dbscan_large.show()

"""###3.23 How does the choice of eps and min_samples impact the clusters? Describe how the clusters change once you adjust multiple versions of that required parameter.
DBSCAN clustering reveals accident hotspots by identifying areas where accidents occur in dense spatial concentrations. By adjusting eps (the neighborhood radius) and min_samples (the density threshold), the shape and number of clusters change substantially. When eps is small, only very tight, compact hotspots appear—typically around extremely busy junctions or intersections. Many points may be classified as noise, meaning they do not belong to any dense region. Increasing eps makes clusters broader, causing nearby hotspots to merge. This may reveal larger spatial patterns along major transport corridors, but it also risks over-generalisation.
Similarly, lowering min_samples makes the algorithm more sensitive, producing more clusters even from moderate levels of accident density. Higher min_samples values filter out smaller accumulations and highlight only the most persistent and meaningful hotspots. When both parameters are increased simultaneously, clusters may become large and smoother, depicting macro-level urban structures rather than specific danger points. Overall, DBSCAN provides a flexible way to uncover accident hotspots, but the interpretation of clusters depends strongly on parameter choice. Proper tuning is critical to balancing sensitivity and meaningfulness.

### 3.24 Briefly reflect on the clusters created using K-Means and the ones generated with DBSCAN. What insights can you gain from that? Do you see any limitations?
K-Means and DBSCAN produce fundamentally different types of clusters. K-Means divides the data into a specified number of clusters (k), forcing all accidents into groups even when no natural cluster exists. This makes it good for capturing broad spatial zones or general patterns but less effective at identifying true hotspots. DBSCAN, on the other hand, finds clusters only where dense groups of accidents naturally occur. It labels sparse points as noise, providing a more realistic representation of where risk is concentrated.
The key insight is that K-Means reflects spatial structure, while DBSCAN reflects spatial intensity. K-Means identifies general regions with similar characteristics, whereas DBSCAN pinpoints high-risk areas such as motorway interchanges, busy city centres, and major junctions. A limitation of K-Means is its sensitivity to k and its inability to detect irregular shapes. DBSCAN’s limitation is parameter sensitivity—small changes in eps can drastically change the cluster boundaries. When used together, however, the two algorithms complement each other, helping us understand both the geography and the density of accidents.

### 3.25 What do you think are the real-world implications of the identified clusters in the field of urban planning?
The clusters identified in this analysis have important implications for urban planning and transport safety management. DBSCAN hotspots can indicate locations where road design, traffic flow patterns, or pedestrian infrastructure may need improvement. These might include intersections with poor visibility, congested motorway exits, or areas lacking safe pedestrian crossings. Understanding the spatial intensity of accidents helps planners prioritise investment in road redesign, signage, lighting, traffic calming, or enforcement.
Meanwhile, the attribute-based K-Means clusters, which incorporate severity and vehicle involvement, offer insights into the nature of accidents rather than just their location. For example, regions with high-severity multi-vehicle accidents may require different interventions from areas dominated by low-severity incidents. Together, the clustering outputs can influence decisions about emergency response placement, speed limit adjustments, bus route optimisation, and long-term street redesign. Ultimately, identifying and interpreting accident clusters helps cities create safer, more efficient, and more resilient transportation systems.
"""